{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from docx import Document\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])  # Extract all paragraphs\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "best_practices_file_path = \"./data/The CyberGov™ Framework – Optimizing Your Cybersecurity Posture v. 8.0 14 Dec 2023.docx\"\n",
    "board_report_file_path = \"./data/Sample board of directors meeting.docx\"\n",
    "board_memo_file_path = \"./data/Board Memo 1 March 14.docx\"\n",
    "\n",
    "best_practices_text = read_docx(best_practices_file_path)\n",
    "board_report_text = read_docx(board_report_file_path)\n",
    "board_memo_text = read_docx(board_memo_file_path)\n",
    "\n",
    "best_practices_doc = Document(best_practices_text, metadata={\"source\":best_practices_file_path})\n",
    "board_report_doc = Document(board_report_text, metadata={\"source\":board_report_file_path})\n",
    "board_memo_doc = Document(board_memo_text, metadata={\"source\":board_memo_file_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "best_practices_doc = Document(best_practices_text, metadata={\"source\":best_practices_file_path})\n",
    "board_report_doc = Document(board_report_text, metadata={\"source\":board_report_file_path})\n",
    "board_memo_doc = Document(board_memo_text, metadata={\"source\":board_memo_file_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Attempt at the Baseline LLM Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document as LCDocument\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API Key\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Split document into smaller chunks for embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_text(board_report_text)\n",
    "\n",
    "# Convert chunks into LangChain Document objects\n",
    "docs = [LCDocument(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Store embeddings in FAISS vector database\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Save FAISS index for later use\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "# Load FAISS index (optional, for retrieval)\n",
    "vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ComplianceReport(BaseModel):\n",
    "    status: str\n",
    "    explanation: str\n",
    "    corrective_measures: str\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define best practice statement\n",
    "practice = \"Practice 4.1: Policies, processes, and procedures for managing cyber breaches internally are established and reviewed at least annually.\"\n",
    "\n",
    "# Retrieve top 5 most relevant documents with similarity scores\n",
    "retrieved_docs = vector_store.similarity_search_with_score(practice, k=5)\n",
    "\n",
    "# Format retrieved documents into a structured prompt\n",
    "context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, (doc, score) in enumerate(retrieved_docs)])\n",
    "\n",
    "# Construct the final prompt\n",
    "final_prompt = f\"\"\"\n",
    "You are an expert compliance analyst tasked with evaluating the compliance status of the best practice based on the provided context. \n",
    "The context consists of relevant remarks from board members. Clearly state the status in your response as \"Pass\" or \"Fail\" at the top.\n",
    "\n",
    "### Best Practice:\n",
    "{practice}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "Based on the context, does the organization comply with this best practice? Provide reasoning if it doesn't and corrective measures. Your description should be easy to comprehend. If you don't find any relevant information, you can state that as well.\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert compliance analyst.\"},\n",
    "        {\"role\": \"user\", \"content\": final_prompt}\n",
    "    ],\n",
    "    response_format=ComplianceReport\n",
    ")\n",
    "\n",
    "pprint(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j Graph Over Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_best_practices(text):\n",
    "    principles = []\n",
    "    current_principle = None\n",
    "    \n",
    "    lines = text.strip().split(\"\\n\")  # Split by lines\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        principle_match = re.match(r\"^Principle (\\d+\\.\\d+): (.+)\", line)\n",
    "        practice_match = re.match(r\"^Practice (\\d+\\.\\d+): (.+)\", line)\n",
    "        \n",
    "        if principle_match:\n",
    "            if current_principle:\n",
    "                principles.append(current_principle)\n",
    "            current_principle = {\n",
    "                \"id\": principle_match.group(1),\n",
    "                \"name\": principle_match.group(2),\n",
    "                \"practices\": []\n",
    "            }\n",
    "        elif practice_match and current_principle:\n",
    "            current_principle[\"practices\"].append({\n",
    "                \"id\": practice_match.group(1),\n",
    "                \"description\": practice_match.group(2)\n",
    "            })\n",
    "    \n",
    "    if current_principle:\n",
    "        principles.append(current_principle)\n",
    "    \n",
    "    return principles\n",
    "\n",
    "principles=parse_best_practices(best_practices_text)\n",
    "\n",
    "# workaround for an anomaly in the data\n",
    "# principles[2]['practices'][1]['id'] = '3.1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of schema queries\n",
    "queries = [\n",
    "    \"\"\"\n",
    "    CREATE CONSTRAINT unique_principle_id IF NOT EXISTS \n",
    "    FOR (p:Principle) REQUIRE p.id IS UNIQUE\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE CONSTRAINT unique_practice_id IF NOT EXISTS \n",
    "    FOR (pr:Practice) REQUIRE pr.id IS UNIQUE\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE CONSTRAINT unique_keyindicator_details IF NOT EXISTS \n",
    "    FOR (ki:KeyIndicator) REQUIRE ki.details IS UNIQUE\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    graph.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting data into Neo4j\n",
    "for principle in principles:\n",
    "    # Ensure Principle node is created or matched\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MERGE (p:Principle {id: $principle_id})\n",
    "        ON CREATE SET p.name = $principle_name\n",
    "        \"\"\",\n",
    "        params={\"principle_id\": principle[\"id\"], \"principle_name\": principle[\"name\"]},\n",
    "    )\n",
    "\n",
    "    for practice in principle[\"practices\"]:\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "            MATCH (p:Principle {id: $principle_id})  // Ensure Principle exists\n",
    "            MERGE (pr:Practice {id: $practice_id})  // Ensure unique Practice by ID\n",
    "            ON CREATE SET pr.description = $practice_desc  // Set description only on creation\n",
    "            MERGE (p)-[:HAS_PRACTICE]->(pr)  // Create relationship\n",
    "            \"\"\",\n",
    "            params={\n",
    "                \"principle_id\": principle[\"id\"],\n",
    "                \"practice_id\": practice[\"id\"],\n",
    "                \"practice_desc\": practice[\"description\"],\n",
    "            },\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'node_count': 42}]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (n) RETURN count(n) AS node_count;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (n) DETACH DELETE n;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key indicators mapped to their corresponding practice IDs (provided by Bob)\n",
    "key_indicators = {\n",
    "    \"2.6\": [\n",
    "        \"How can we guarantee that all subsidiaries fully implement cybersecurity communication channels?\",\n",
    "        \"What barriers might delay the complete deployment of these communication frameworks, and how can they be mitigated?\",\n",
    "        \"How do we foster greater trust among suppliers and third parties to encourage transparency in cybersecurity risk sharing?\",\n",
    "        \"Could leveraging contractual obligations improve data-sharing practices with external partners?\",\n",
    "    ],\n",
    "    \"3.4\": [\n",
    "        \"What mechanisms can be implemented to extend supply chain cybersecurity risk management to international vendors?\",\n",
    "        \"What challenges might arise from merging cybersecurity risk with enterprise risk management, and how can they be resolved?\",\n",
    "        \"How can board members be encouraged to perceive cybersecurity as a key component of corporate governance rather than a standalone function?\",\n",
    "    ],\n",
    "    \"4.5\": [\n",
    "        \"How can we better model cyber risks to enhance response planning in unpredictable scenarios?\",\n",
    "        \"What initiatives can be introduced to align staff and board perspectives on a unified incident response strategy?\",\n",
    "        \"How can we ensure that real-world data collection is comprehensive and accessible across all business units?\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Insert key indicators and establish relationships\n",
    "for practice_id, questions in key_indicators.items():\n",
    "    for question in questions:\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "            MATCH (pr:Practice {id: $practice_id})\n",
    "            MERGE (ki:KeyIndicator {question: $question})\n",
    "            MERGE (pr)-[:HAS_KEY_INDICATOR]->(ki)\n",
    "            \"\"\",\n",
    "            params={\"practice_id\": practice_id, \"question\": question},\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve principle and related practices along with key indicators (if any)\n",
    "principle_id = \"2.0\"\n",
    "\n",
    "result = graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (p:Principle)-[:HAS_PRACTICE]->(pr:Practice)-[:HAS_KEY_INDICATOR]->(ki:KeyIndicator)\n",
    "    WHERE p.id = $principle_id\n",
    "    RETURN p, pr, ki;\n",
    "    \"\"\",\n",
    "    params={\"principle_id\": principle_id}\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Hybrid Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_and_print_key_indicators(graph, principle_id):\n",
    "    result = graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (p:Principle)-[:HAS_PRACTICE]->(pr:Practice)-[:HAS_KEY_INDICATOR]->(ki:KeyIndicator)\n",
    "        WHERE p.id = $principle_id\n",
    "        RETURN p, pr, ki;\n",
    "        \"\"\",\n",
    "        params={\"principle_id\": principle_id}\n",
    "    )\n",
    "\n",
    "    for record in result:\n",
    "        # principle = record[\"p\"]\n",
    "        practice = record[\"pr\"]\n",
    "        key_indicator = record[\"ki\"]\n",
    "\n",
    "        if key_indicator:\n",
    "            # print(f\"Principle: {principle['name']}\")\n",
    "            print(f\"  Practice: {practice['id']}\")\n",
    "            print(f\"    Key Indicator: {key_indicator['question']}\")\n",
    "\n",
    "for principle in principles:\n",
    "    traverse_and_print_key_indicators(graph, principle[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document as LCDocument\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API Key\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Split document into smaller chunks for embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_text(board_memo_text)\n",
    "\n",
    "# Convert chunks into LangChain Document objects\n",
    "docs = [LCDocument(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Store embeddings in FAISS vector database\n",
    "vector_store_memo = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# # Save FAISS index for later use\n",
    "# vector_store_memo.save_local(\"faiss_index_memo\")\n",
    "\n",
    "# # Load FAISS index (optional, for retrieval)\n",
    "# vector_store_memo = FAISS.load_local(\"faiss_index_memo\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import json\n",
    "\n",
    "\n",
    "class ComplianceReport(BaseModel):\n",
    "    status: str = Field(description=\"The compliance status: 'Pass' or 'Fail'\")\n",
    "    causality: str = Field(description=\"A to-the-point concise reason (Cause) for the compliance status (Effect)\")\n",
    "    explanation: str = Field(description=\"Explanation of the compliance status finding\")\n",
    "    corrective_measures: str = Field(\n",
    "        description=\"Suggested corrective measures if status is 'Fail', or empty if 'Pass'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a parser for the ComplianceReport model\n",
    "parser = PydanticOutputParser(pydantic_object=ComplianceReport)\n",
    "\n",
    "# Define the prompt template with instructions for JSON formatting\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert compliance analyst tasked with evaluating the compliance status of the best practice based on the provided context. \n",
    "The context consists of relevant remarks from board members. Clearly state the status in your response as \"Pass\" or \"Fail\" at the top.\n",
    "You will be provided with a key indicator and a practice statement. You need to evaluate the compliance status of the practice based on the key indicator.\n",
    "\n",
    "### Best Practice:\n",
    "{practice}\n",
    "\n",
    "### Key Indicator:\n",
    "{key_indicator}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "Based on the context, does the organization comply with this best practice? Provide reasoning if it doesn't and corrective measures. Your description should be easy to comprehend. If you don't find any relevant information, you can state that as well.\n",
    "\n",
    "Format your response as a JSON object with the following fields:\n",
    "- status: \"Pass\" or \"Fail\"\n",
    "- causality: A to-the-point concise reason (Cause) for the compliance status (Effect)\n",
    "- explanation: A detailed explanation of why the organization passes or fails\n",
    "- corrective_measures: Suggested actions if failing, or empty string if passing\n",
    "\n",
    "### Answer:\n",
    "\"\"\",\n",
    "    input_variables=[\"practice\", \"key_indicator\", \"context\"],\n",
    ")\n",
    "\n",
    "# Initialize the LLM with JSON output format\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\", model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    ")\n",
    "\n",
    "# Create the compliance chain\n",
    "compliance_chain = LLMChain(\n",
    "    llm=llm, prompt=prompt_template, output_key=\"compliance_report\"\n",
    ")\n",
    "\n",
    "\n",
    "# Function to run the compliance check with vector retrieval\n",
    "def check_compliance(practice_statement, key_indicator, vector_store, k=5):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = vector_store.similarity_search_with_score(practice_statement, k=k)\n",
    "\n",
    "    # Format retrieved documents into a structured context\n",
    "    context = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Document {i+1} (score: {score}):\\n{doc.page_content}\"\n",
    "            for i, (doc, score) in enumerate(retrieved_docs)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Run the compliance chain\n",
    "    result = compliance_chain.invoke(\n",
    "        {\"practice\": practice_statement, \"key_indicator\": key_indicator, \"context\": context}\n",
    "    )\n",
    "\n",
    "    # Parse the JSON string into a ComplianceReport object\n",
    "    try:\n",
    "        json_str = result[\"compliance_report\"]\n",
    "        parsed_json = json.loads(json_str)\n",
    "        return ComplianceReport(**parsed_json)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse result into ComplianceReport model. Error: {e}\")\n",
    "        print(\"Raw result:\")\n",
    "        pprint(result[\"compliance_report\"])\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "practice = \"Practice 4.1: Policies, processes, and procedures for managing cyber breaches internally are established and reviewed at least annually.\"\n",
    "key_indicator = \"Not Provided\"\n",
    "report = check_compliance(practice, key_indicator, vector_store_memo, k=5)\n",
    "if report:\n",
    "    print(f\"Status: {report.status}\")\n",
    "    print(f\"Cause: {report.causality}\")\n",
    "    print(f\"Explanation: {report.explanation}\")\n",
    "    print(f\"Corrective measures: {report.corrective_measures}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for principle in principles:\n",
    "    result = graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (p:Principle)-[:HAS_PRACTICE]->(pr:Practice)-[:HAS_KEY_INDICATOR]->(ki:KeyIndicator)\n",
    "        WHERE p.id = $principle_id\n",
    "        RETURN p, pr, ki;\n",
    "        \"\"\",\n",
    "        params={\"principle_id\": principle[\"id\"]},\n",
    "    )\n",
    "\n",
    "    for record in result:\n",
    "        # principle = record[\"p\"]\n",
    "        practice = record[\"pr\"]\n",
    "        key_indicator = record[\"ki\"]\n",
    "\n",
    "        if key_indicator:\n",
    "            # print(f\"Principle: {principle['name']}\")\n",
    "            print(f\"**Practice:** {practice['id']}\")\n",
    "            print(f\"**Key Indicator:** {key_indicator['question']}\")\n",
    "\n",
    "            report = check_compliance(practice[\"description\"], key_indicator[\"question\"], vector_store_memo)\n",
    "            if report:\n",
    "                print(f\"**Status:** {report.status}\")\n",
    "                print(f\"**Cause:** {report.causality}\")\n",
    "                print(f\"**Explanation:** {report.explanation}\")\n",
    "                print(f\"**Corrective measures:** {report.corrective_measures}\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation with confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import json\n",
    "\n",
    "\n",
    "class ComplianceReport(BaseModel):\n",
    "    status: str = Field(description=\"The compliance status: 'Pass' or 'Fail'\")\n",
    "\n",
    "# Create a parser for the ComplianceReport model\n",
    "parser = PydanticOutputParser(pydantic_object=ComplianceReport)\n",
    "\n",
    "# Define the prompt template with instructions for JSON formatting\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert compliance analyst tasked with evaluating the compliance status of the best practice based on the provided context. \n",
    "The context consists of relevant remarks from board members. Clearly state the status in your response as \"Pass\" or \"Fail\" at the top.\n",
    "You will be provided with a key indicator and a practice statement. You need to evaluate the compliance status of the practice based on the key indicator.\n",
    "\n",
    "### Best Practice:\n",
    "{practice}\n",
    "\n",
    "### Key Indicator:\n",
    "{key_indicator}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "Based on the context, does the organization comply with this best practice? If you don't find any relevant information, you can state that as \"Fail\".\n",
    "\n",
    "Format your response as a JSON object with the following fields:\n",
    "- status: \"Pass\" or \"Fail\"\n",
    "\n",
    "### Answer:\n",
    "\"\"\",\n",
    "    input_variables=[\"practice\", \"key_indicator\", \"context\"],\n",
    ")\n",
    "\n",
    "# Initialize the LLM with JSON output format\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\", logprobs=True, model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    ")\n",
    "\n",
    "compliance_chain = prompt_template | llm\n",
    "\n",
    "\n",
    "# Function to run the compliance check with vector retrieval\n",
    "def check_compliance(practice_statement, key_indicator, vector_store, k=5):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = vector_store.similarity_search_with_score(practice_statement, k=k)\n",
    "\n",
    "    # Format retrieved documents into a structured context\n",
    "    context = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Document {i+1} (score: {score}):\\n{doc.page_content}\"\n",
    "            for i, (doc, score) in enumerate(retrieved_docs)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Run the compliance chain\n",
    "    result = compliance_chain.invoke(\n",
    "        {\"practice\": practice_statement, \"key_indicator\": key_indicator, \"context\": context}\n",
    "    )\n",
    "\n",
    "    return result\n",
    "    # # Parse the JSON string into a ComplianceReport object\n",
    "    # try:\n",
    "    #     json_str = result[\"compliance_report\"]\n",
    "    #     parsed_json = json.loads(json_str)\n",
    "    #     return ComplianceReport(**parsed_json)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Failed to parse result into ComplianceReport model. Error: {e}\")\n",
    "    #     print(\"Raw result:\")\n",
    "    #     pprint(result[\"compliance_report\"])\n",
    "    #     return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "practice = \"Practice 4.1: Policies, processes, and procedures for managing cyber breaches internally are established and reviewed at least annually.\"\n",
    "key_indicator = \"Not Provided\"\n",
    "report = check_compliance(practice, key_indicator, vector_store_memo, k=5)\n",
    "# if report:\n",
    "#     print(f\"Status: {report.status}\")\n",
    "#     print(f\"Explanation: {report.explanation}\")\n",
    "#     print(f\"Corrective measures: {report.corrective_measures}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "parser.parse(report.content)\n",
    "for logprob in report.response_metadata['logprobs']['content']:\n",
    "    if logprob['token'] == 'Pass' or logprob['token'] == 'Fail':\n",
    "        print(f\"Prob('{logprob['token']}'): {np.exp(logprob['logprob'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for principle in principles:\n",
    "    result = graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (p:Principle)-[:HAS_PRACTICE]->(pr:Practice)-[:HAS_KEY_INDICATOR]->(ki:KeyIndicator)\n",
    "        WHERE p.id = $principle_id\n",
    "        RETURN p, pr, ki;\n",
    "        \"\"\",\n",
    "        params={\"principle_id\": principle[\"id\"]},\n",
    "    )\n",
    "\n",
    "    for record in result:\n",
    "        # principle = record[\"p\"]\n",
    "        practice = record[\"pr\"]\n",
    "        key_indicator = record[\"ki\"]\n",
    "\n",
    "        if key_indicator:\n",
    "            # print(f\"Principle: {principle['name']}\")\n",
    "            print(f\"**Practice:** {practice['id']}\")\n",
    "            print(f\"**Key Indicator:** {key_indicator['question']}\")\n",
    "\n",
    "            report = check_compliance(practice[\"description\"], key_indicator[\"question\"], vector_store_memo)\n",
    "            for logprob in report.response_metadata['logprobs']['content']:\n",
    "                if logprob['token'] == 'Pass' or logprob['token'] == 'Fail':\n",
    "                    confidence = f\"Prob('{logprob['token']}'): {np.exp(logprob['logprob']):.4f}\"\n",
    "            report = parser.parse(report.content)\n",
    "            \n",
    "            print(f\"**Status:** {report.status}\")\n",
    "            print(confidence)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize OpenAI model with logprobs enabled\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\",\n",
    "                 temperature=0,\n",
    "                 openai_api_key=openai_api_key,\n",
    "                 model_kwargs={\"logprobs\": True})  # Request log probabilities\n",
    "\n",
    "# Define a prompt\n",
    "message = HumanMessage(content=\"What is the capital of France?\")\n",
    "\n",
    "# Get response with log probabilities\n",
    "response = llm([message])\n",
    "\n",
    "# Print full response including log probabilities\n",
    "# pprint(response.response_metadata)\n",
    "\n",
    "# Extract the log probabilities from the response\n",
    "response_logprobs = response.response_metadata[\"logprobs\"][\"content\"]\n",
    "pprint(response_logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from llm_confidence.logprobs_handler import LogprobsHandler\n",
    "\n",
    "# Initialize the LogprobsHandler\n",
    "logprobs_handler = LogprobsHandler()\n",
    "\n",
    "def get_completion(\n",
    "        messages: list[dict[str, str]],\n",
    "        model: str = \"gpt-4o\",\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        stop=None,\n",
    "        seed=42,\n",
    "        response_format=None,\n",
    "        logprobs=None,\n",
    "        top_logprobs=None,\n",
    "):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if response_format:\n",
    "        params[\"response_format\"] = response_format\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion\n",
    "\n",
    "# Set up your OpenAI client with your API key\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Define a prompt for completion\n",
    "response_raw = get_completion(\n",
    "    [{'role': 'user', 'content': 'Tell me the name of capital of Pakistan, and return the response in JSON format.'}],\n",
    "    logprobs=True,\n",
    "    response_format={'type': 'json_object'}\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(response_raw.choices[0].message.content)\n",
    "\n",
    "# Extract the log probabilities from the response\n",
    "response_logprobs = response_raw.choices[0].logprobs.content if hasattr(response_raw.choices[0], 'logprobs') else []\n",
    "\n",
    "# Format the logprobs\n",
    "logprobs_formatted = logprobs_handler.format_logprobs(response_logprobs)\n",
    "\n",
    "# Process the log probabilities to get confidence scores\n",
    "confidence = logprobs_handler.process_logprobs(\n",
    "    logprobs_formatted, \n",
    ")\n",
    "\n",
    "# Print the confidence scores\n",
    "print(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_formatted\n",
    "# logprobs_handler.calculate_words_probas(logprobs_formatted)\n",
    "response_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_formatted = []\n",
    "for logprob in response_logprobs:\n",
    "    logprob_formatted = {'token': logprob.token, 'logprob': logprob.logprob,\n",
    "                            'log_topprobs': [{'token': log_topprob.token, 'logprob': log_topprob.logprob}\n",
    "                                            for log_topprob in logprob.top_logprobs]}\n",
    "    logprobs_formatted.append(logprob_formatted)\n",
    "logprobs_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "probas_df = pd.DataFrame({'token': [i['token'] for i in logprobs_formatted],\n",
    "                                  'logprob': [i['logprob'] for i in logprobs_formatted]})\n",
    "\n",
    "# Combine tokens into key-value pairs\n",
    "# Assuming tokens that form key-value pairs are sequential\n",
    "key_value_pairs = []\n",
    "current_pair = []\n",
    "for idx, row in probas_df.iterrows():\n",
    "    token = str(row['token'])\n",
    "    if token.strip() != '' and not token.strip() in ['{', '}']:\n",
    "        current_pair.append(idx)\n",
    "    # Check if the token likely ends a key-value pair\n",
    "    if token.endswith(',\\n') or token.endswith(']\\n') or token.strip().endswith(',') or token.strip().endswith(\n",
    "            '}') or token.endswith('\"}') or token.endswith(\"'}\") or token.endswith(',\"') or token.endswith(\n",
    "        \",'\") or token.endswith('\",\\n') or token.endswith(\"',\\n\"):\n",
    "        if len(current_pair) > 0:\n",
    "            key_value_pairs.append(current_pair)\n",
    "        current_pair = []\n",
    "\n",
    "# Calculate key-value pair probabilities\n",
    "pair_probs = []\n",
    "for pair in key_value_pairs:\n",
    "    print(probas_df.loc[pair, 'token'])\n",
    "    pair_logprob = probas_df.loc[pair, 'logprob'].sum()\n",
    "    print(pair_logprob)\n",
    "    pair_prob = np.exp(pair_logprob)\n",
    "    pair_probs.append((''.join(probas_df.loc[pair, 'token']), pair_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
